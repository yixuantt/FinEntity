{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T05:48:48.537247400Z",
     "start_time": "2023-06-01T05:48:48.436252400Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load The Data\n",
    "import json\n",
    "\n",
    "raw = json.load(open('./data/FinEntity.json'))\n",
    "\n",
    "\n",
    "# data = json.load(open('./data/urop_annotation_4000_annotations.json'))\n",
    "# raw = data.get(\"examples\")\n",
    "# for example in raw:\n",
    "#     for annotation in example['annotations']:\n",
    "#         #We expect the key of label to be label but the data has tag\n",
    "#         annotation['label'] = annotation['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Neutral', 2: 'I-Neutral', 3: 'L-Neutral', 4: 'U-Neutral', 5: 'B-Positive', 6: 'I-Positive', 7: 'L-Positive', 8: 'U-Positive', 9: 'B-Negative', 10: 'I-Negative', 11: 'L-Negative', 12: 'U-Negative'}\n",
      "13\n",
      "987\n"
     ]
    }
   ],
   "source": [
    "## Preparing Sequence Labeling Data for Transformers\n",
    "from sequence_aligner.labelset import LabelSet\n",
    "from sequence_aligner.dataset import TrainingDataset\n",
    "from sequence_aligner.containers import TraingingBatch\n",
    "from transformers import BertTokenizerFast\n",
    "#bert-base-cased yiyanghkust/finbert-pretrain\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "label_set = LabelSet(labels=[\"Neutral\", \"Positive\", \"Negative\"])  # label in this dataset\n",
    "print(label_set.ids_to_label)\n",
    "print(len(label_set.ids_to_label.values()))\n",
    "dataset = TrainingDataset(data=raw, tokenizer=tokenizer, label_set=label_set,tokens_per_batch = 128)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 1103, 3112, 1334, 117, 24824, 1110, 11158, 1158, 127, 110, 1170, 170, 11147, 1107, 21081, 3791, 1105, 17019, 11166, 27158, 20284, 8552, 1110, 1145, 1146, 1118, 127, 110, 1170, 5022, 3222, 11471, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "On the positive side, Siemens is rallying 6 % after a boom in quarterly orders and packaging maker Huhtamaki is also up by 6 % after profit beat expectations. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "{0: 'O', 1: 'B-Neutral', 2: 'I-Neutral', 3: 'L-Neutral', 4: 'U-Neutral', 5: 'B-Positive', 6: 'I-Positive', 7: 'L-Positive', 8: 'U-Positive', 9: 'B-Negative', 10: 'I-Negative', 11: 'L-Negative', 12: 'U-Negative'}\n"
     ]
    }
   ],
   "source": [
    "## Prepare train data and valid data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "validate_size = len(dataset) - train_size\n",
    "train_dataset, validate_dataset = random_split(dataset, [train_size, validate_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=TraingingBatch, shuffle=True, )\n",
    "val_loader = DataLoader(validate_dataset, batch_size=16, collate_fn=TraingingBatch, shuffle=True, )\n",
    "\n",
    "print(dataset[1].input_ids)\n",
    "print(dataset[1].labels)\n",
    "print(dataset[1].attention_masks)\n",
    "print(tokenizer.decode(dataset[1].input_ids))\n",
    "print(dataset.label_set.ids_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertSoftmaxForNer: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertSoftmaxForNer from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertSoftmaxForNer from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertSoftmaxForNer were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======START TRAIN EPOCHS 1=======\n",
      "Epoch: 1, train Loss:0.6588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00       105\n",
      "     Neutral       0.43      0.08      0.14       222\n",
      "    Positive       0.00      0.00      0.00       105\n",
      "\n",
      "   micro avg       0.43      0.04      0.08       432\n",
      "   macro avg       0.14      0.03      0.05       432\n",
      "weighted avg       0.22      0.04      0.07       432\n",
      "\n",
      "=======START TRAIN EPOCHS 2=======\n",
      "Epoch: 2, train Loss:0.1878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.05      0.09       105\n",
      "     Neutral       0.71      0.59      0.65       222\n",
      "    Positive       0.48      0.50      0.49       105\n",
      "\n",
      "   micro avg       0.63      0.44      0.52       432\n",
      "   macro avg       0.61      0.38      0.41       432\n",
      "weighted avg       0.64      0.44      0.47       432\n",
      "\n",
      "=======START TRAIN EPOCHS 3=======\n",
      "Epoch: 3, train Loss:0.0944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.65      0.69       105\n",
      "     Neutral       0.74      0.69      0.72       222\n",
      "    Positive       0.86      0.41      0.55       105\n",
      "\n",
      "   micro avg       0.76      0.61      0.68       432\n",
      "   macro avg       0.78      0.58      0.65       432\n",
      "weighted avg       0.77      0.61      0.67       432\n",
      "\n",
      "=======START TRAIN EPOCHS 4=======\n",
      "Epoch: 4, train Loss:0.0485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.81      0.81       105\n",
      "     Neutral       0.78      0.73      0.76       222\n",
      "    Positive       0.83      0.70      0.76       105\n",
      "\n",
      "   micro avg       0.80      0.74      0.77       432\n",
      "   macro avg       0.80      0.74      0.77       432\n",
      "weighted avg       0.80      0.74      0.77       432\n",
      "\n",
      "=======START TRAIN EPOCHS 5=======\n",
      "Epoch: 5, train Loss:0.0264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.81      0.83       105\n",
      "     Neutral       0.80      0.75      0.77       222\n",
      "    Positive       0.76      0.73      0.75       105\n",
      "\n",
      "   micro avg       0.80      0.76      0.78       432\n",
      "   macro avg       0.80      0.76      0.78       432\n",
      "weighted avg       0.80      0.76      0.78       432\n",
      "\n",
      "=======START TRAIN EPOCHS 6=======\n",
      "Epoch: 6, train Loss:0.0150\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.87      0.86       105\n",
      "     Neutral       0.78      0.79      0.78       222\n",
      "    Positive       0.89      0.71      0.79       105\n",
      "\n",
      "   micro avg       0.82      0.79      0.81       432\n",
      "   macro avg       0.84      0.79      0.81       432\n",
      "weighted avg       0.83      0.79      0.80       432\n",
      "\n",
      "=======START TRAIN EPOCHS 7=======\n",
      "Epoch: 7, train Loss:0.0106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.85      0.86       105\n",
      "     Neutral       0.81      0.73      0.76       222\n",
      "    Positive       0.84      0.78      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.77      0.80       432\n",
      "   macro avg       0.84      0.78      0.81       432\n",
      "weighted avg       0.83      0.77      0.80       432\n",
      "\n",
      "=======START TRAIN EPOCHS 8=======\n",
      "Epoch: 8, train Loss:0.0072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.84      0.87       105\n",
      "     Neutral       0.77      0.81      0.79       222\n",
      "    Positive       0.90      0.70      0.78       105\n",
      "\n",
      "   micro avg       0.83      0.79      0.81       432\n",
      "   macro avg       0.86      0.78      0.81       432\n",
      "weighted avg       0.83      0.79      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 9=======\n",
      "Epoch: 9, train Loss:0.0052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.82      0.86       105\n",
      "     Neutral       0.82      0.78      0.80       222\n",
      "    Positive       0.80      0.81      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.81       432\n",
      "   macro avg       0.84      0.80      0.82       432\n",
      "weighted avg       0.83      0.80      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 10=======\n",
      "Epoch: 10, train Loss:0.0041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.82      0.86       105\n",
      "     Neutral       0.82      0.78      0.80       222\n",
      "    Positive       0.78      0.83      0.80       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.81       432\n",
      "   macro avg       0.83      0.81      0.82       432\n",
      "weighted avg       0.83      0.80      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 11=======\n",
      "Epoch: 11, train Loss:0.0031\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.82      0.86       105\n",
      "     Neutral       0.82      0.76      0.79       222\n",
      "    Positive       0.75      0.82      0.79       105\n",
      "\n",
      "   micro avg       0.82      0.79      0.81       432\n",
      "   macro avg       0.83      0.80      0.81       432\n",
      "weighted avg       0.82      0.79      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 12=======\n",
      "Epoch: 12, train Loss:0.0024\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.83      0.86       105\n",
      "     Neutral       0.81      0.77      0.79       222\n",
      "    Positive       0.86      0.80      0.83       105\n",
      "\n",
      "   micro avg       0.84      0.79      0.82       432\n",
      "   macro avg       0.86      0.80      0.83       432\n",
      "weighted avg       0.84      0.79      0.82       432\n",
      "\n",
      "=======START TRAIN EPOCHS 13=======\n",
      "Epoch: 13, train Loss:0.0021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.83      0.86       105\n",
      "     Neutral       0.80      0.80      0.80       222\n",
      "    Positive       0.81      0.79      0.80       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.81       432\n",
      "   macro avg       0.84      0.81      0.82       432\n",
      "weighted avg       0.83      0.80      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 14=======\n",
      "Epoch: 14, train Loss:0.0019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.85      0.86       105\n",
      "     Neutral       0.82      0.76      0.79       222\n",
      "    Positive       0.82      0.81      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.79      0.81       432\n",
      "   macro avg       0.83      0.80      0.82       432\n",
      "weighted avg       0.83      0.79      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 15=======\n",
      "Epoch: 15, train Loss:0.0020\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.83      0.86       105\n",
      "     Neutral       0.81      0.79      0.80       222\n",
      "    Positive       0.83      0.79      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.82       432\n",
      "   macro avg       0.84      0.80      0.82       432\n",
      "weighted avg       0.83      0.80      0.82       432\n",
      "\n",
      "=======START TRAIN EPOCHS 16=======\n",
      "Epoch: 16, train Loss:0.0018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.83      0.86       105\n",
      "     Neutral       0.81      0.78      0.79       222\n",
      "    Positive       0.83      0.79      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.81       432\n",
      "   macro avg       0.84      0.80      0.82       432\n",
      "weighted avg       0.83      0.80      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 17=======\n",
      "Epoch: 17, train Loss:0.0017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.83      0.86       105\n",
      "     Neutral       0.81      0.78      0.79       222\n",
      "    Positive       0.83      0.79      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.81       432\n",
      "   macro avg       0.84      0.80      0.82       432\n",
      "weighted avg       0.83      0.80      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 18=======\n",
      "Epoch: 18, train Loss:0.0018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.83      0.86       105\n",
      "     Neutral       0.81      0.78      0.79       222\n",
      "    Positive       0.83      0.79      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.81       432\n",
      "   macro avg       0.84      0.80      0.82       432\n",
      "weighted avg       0.83      0.80      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 19=======\n",
      "Epoch: 19, train Loss:0.0016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.83      0.86       105\n",
      "     Neutral       0.81      0.78      0.79       222\n",
      "    Positive       0.83      0.79      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.81       432\n",
      "   macro avg       0.84      0.80      0.82       432\n",
      "weighted avg       0.83      0.80      0.81       432\n",
      "\n",
      "=======START TRAIN EPOCHS 20=======\n",
      "Epoch: 20, train Loss:0.0016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.83      0.86       105\n",
      "     Neutral       0.81      0.78      0.79       222\n",
      "    Positive       0.83      0.79      0.81       105\n",
      "\n",
      "   micro avg       0.83      0.80      0.81       432\n",
      "   macro avg       0.84      0.80      0.82       432\n",
      "weighted avg       0.83      0.80      0.81       432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Bert \n",
    "import warnings\n",
    "from model.bert_crf import BertSoftmaxForNer\n",
    "from transformers import get_linear_schedule_with_warmup,BertConfig,BertForTokenClassification\n",
    "from torch import cuda\n",
    "import config\n",
    "import numpy as np\n",
    "from util.train import train_epoch, valid_epoch_not_crf\n",
    "from  torch.optim import AdamW\n",
    "import torch \n",
    "from util.process import ids_to_labels,Metrics,Metrics_e\n",
    "warnings.filterwarnings('ignore')\n",
    "model = BertSoftmaxForNer.from_pretrained('bert-base-cased', num_labels=len(label_set.ids_to_label.values()))\n",
    "\n",
    "device = 'cuda:0' if cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "len_dataset = len(train_dataset)\n",
    "t_total = len(train_dataset)\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": config.weight_decay,},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr_crf, eps=1e-6)\n",
    "warmup_steps = int(t_total * config.warm_up_ratio)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=t_total)\n",
    "\n",
    "EPOCHS = config.epoch_num  \n",
    "for e in range(EPOCHS):\n",
    "    print(\"=======START TRAIN EPOCHS %d=======\" %(e+1))\n",
    "    train_loss = train_epoch(e, model, train_loader, optimizer, scheduler,device)\n",
    "\n",
    "    valid_epoch_not_crf(e, model, val_loader,device,label_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
